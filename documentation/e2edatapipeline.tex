\documentclass[a4paper,12pt]{article}

% Author information
\author{Eugen-Cristian RAVARIU, Vladislav TIFTILOV}

% Package imports
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{cite} % For bibliography

% Times New Roman font
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{mathptmx}

% Title and subtitle formatting
\usepackage{titlesec}

% Line spacing
\setstretch{1.15}

% Figure, table, and algorithm numbering
\renewcommand{\thefigure}{\arabic{figure}}
\renewcommand{\thetable}{\arabic{table}}

% Document starts
\begin{document}

% Title Section
\begin{titlepage}
    \centering
    National University of Science and Technology Politehnica Bucharest\\
    \vfill
    {\bfseries\fontsize{14pt}{14pt} DataDot}\par
    \vspace{0.5cm}
    {\bfseries\fontsize{12pt}{12pt} End-to-End Data Pipeline}\par
    \vspace{2.5cm}
    Eugen-Cristian RAVARIU, Vladislav TIFTILOV\\
    \href{https://github.com/vladtf/datadot-e2e-data-pipeline}{datadot-e2e-data-pipeline (github.com)}\par
    \vfill
    \date{\today}
\end{titlepage}

% Abstract Section
\section{Abstract}
\label{sec:abstract}

Fast growth across all domains of our daily life has determined the need for storing and processing 
large amounts of data. The challenges a developer faces are not only caused by the amount of data, 
but also by its variety and quality. Sometimes, the most challenging part is to semantically understand 
the data and to extract valuable information from it.

This paper presents an end-to-end data pipeline that aims to automate the process of data transformation 
and visualization. During the project, we discovered the importance of data quality, which led us to change 
the dataset several times in order to get the most meaningful insights. To face real challenges of the 
big data world, we purposely chose a dataset that is both large and complex. The dataset contains 
information about movies, including the title, genre, rating, runtime, and the number of votes.

We started by cleaning the data and aggregating the information, then we performed statistical 
analysis to generate insights. The decision to use Azure was made because we wanted our solution 
to be end-to-end from data ingestion to visualization on a single cloud platform. Another reason 
was the need for scalability, as the dataset contains over 8GB of data, and Azure provides the 
necessary resources to handle large amounts of data. The data pipeline was implemented using 
Azure Data Factory, Azure Databricks, Azure Synapse Analytics, and Power BI.

The decision to go with a movies dataset was made because we wanted to analyze trends in 
the movie industry, but also due to the complexity of the dataset, which allowed us to gain 
insights across multiple dimensions, such as genre, rating, runtime, and region.

\newpage
\tableofcontents  % Generates the Table of Contents
\newpage

% Main Content
\section{Introduction}
\label{sec:introduction}

Movies are one of the most popular forms of entertainment, enjoyed by millions of people worldwide. 
This project focuses on analyzing a dataset that includes information about movies, such as their titles, 
genres, ratings, runtimes, and number of votes. The goal is to understand how movies are distributed based 
on their genre, rating, runtime, and region. This analysis will help identify trends in the movie industry, 
like the most popular genres and regions, as well as the typical runtimes. The findings will be shown in a 
Power BI dashboard, making it easy to explore the data and learn more about the movie industry.

The project has some challenges, mainly because the dataset is very large and needs an automated way 
to handle it. The dataset is over 8GB in size and is split into several CSV files. To work with this data, 
we need to clean it, combine the information, and analyze it to find useful insights. We will use tools like 
Azure Data Factory, Azure Databricks, Azure Synapse Analytics, and Power BI to create a system that can handle 
these tasks automatically and produce a dashboard for easy visualization.

\section{Opinion}
\label{sec:opinion}

In this section we will present our opinions on the project, the challenges we faced, and the lessons we learned.

\subsection{Eugen-Cristian RAVARIU}

In this project, my primary responsibilities included data ingestion and preprocessing. I utilized Azure services such as Azure Data Lake Gen2, Azure Databricks, and Azure Synapse to accomplish these tasks.
To begin, I set up a storage account with two containers: "bronze" for raw data and "silver" for preprocessed data ready for analysis. Using Azure Databricks, I executed the project's notebooks. 
Databricks offers several advantages, including the ability to create compute clusters of varying types and sizes that scale based on workload demand. 
Additionally, Azure only charges for the cluster while it is running, and notebooks can automatically start clusters when triggered, such as during an Azure Data Factory pipeline.

First, I mounted the two containers in Databricks to enable subsequent notebooks to read from and write to them. I then downloaded the data from Kaggle and stored it in the "bronze" container as five tables. 
In a separate notebook, I read these tables from the "bronze" container and performed preprocessing tasks. 
These included renaming columns to snake case, converting appropriate columns to numeric data types, and splitting array columns into new tables. 
For example, I processed a table containing columns for the film ID and the list of genres by creating a new table with one row for each genre in the list, paired with the corresponding film ID, and removing the column with the genre list. 
This preprocessing step expanded the original five tables into eight, which were saved in the "silver" container.

The final preprocessing step involved creating SQL views for use in Power BI. 
I used Azure Synapse to set up a pipeline that processes each table in the "silver" container and generates the corresponding SQL views.

One significant challenge during this project was managing resource access in Azure. 
Each Azure resource is governed by a set of roles, with each role granting specific permissions. 
To utilize required features, users or resources must be assigned the appropriate roles. 
Adhering to the principle of least privilege, I ensured that each module had only the minimum level of access necessary. 
This approach enhanced both the security and simplicity of the application.

In conclusion, this project provided valuable insights into the practical use of Azure services for data ingestion, preprocessing, and preparation for analysis. 
Through this experience, I honed my skills in using Azure tools like Data Lake, Databricks, and Synapse, as well as in implementing scalable and robust data workflows. 
These learnings will undoubtedly contribute to future projects requiring advanced data processing and analysis capabilities.


\subsection{Vladislav TIFTILOV}

We live in an era where data is the most valuable asset. Yet, the data itself is not valuable without the insights we can extract from it.

During this project, we purposed ourselves to to learn how to build an end-to-end data pipeline, from data ingestion to visualization, but
to make the project more meaningful, we also wanted to extract valuable insights from the data. To be able to fulfill this goal, we started
by choosing a dataset that is both large and complex. We started with a stock market dataset, but we quickly realized that the dataset was
did not contain insights worth extracting. We then switched to a movies dataset, which was more complex and allowed us to extract insights
by corelating data across multiple dimensions.

My contribution to the project was to implement the data analysis and visualization part. I used the data that was preprocessed by my colleague
and stored in Azure Synapse Analytics to create views and extract insights. I then used Power BI to create a dashboard that visualizes the insights.
What I learned while build the visualizations is that the most meaningful insights are extracted when data is corelated across multiple dimensions.
For example, by corelating the rating with the runtime, we were able to see that the average rating peaks at 40 minutes, then dips to 6 at 85 minutes,
and gradually rises to 7.45 at 285 minutes. This kind of insights are only possible when data is analyzed across multiple dimensions.

Another thing that I learned is that cloud providers like Azure provide the necessary tools to build end-to-end data pipelines. The best
part of using a cloud provider in this case it that all the tools are integrated and work seamlessly together. This allowed us to build
the entire project using only Platform-as-a-Service (PaaS) tools, without having to worry about infrastructure or maintenance. Toghether with
integration, Azure also provides scalability, which is essential when working with large datasets. Also, a big advantage of using Azure is
that it provides authentication and authorization out of the box, which makes it easy to manage access to the data. These features allowed us
to be able to collaborate on the same resources by giving role based access to the resources, without having to give access to the entire
Azure subscription. Similar with the communication between the tools, it allowed us to easily configure identities to manage access to the
data, without having to worry about the underlying infrastructure.

What is worth mentioning, are the challenges we faced during the project. The biggest challenge was to understand the data and extract but also
to build the pipeline in a way that the data flow is fully automated end-to-end, which in most of the cases is the most important part of an
data pipeline~\cite{modelling-data-pipelines}. The data was complex and required multiple transformations
to be able to extract meaningful insights. Another challenge was to build the pipeline in a way that it is scalable and can handle large amounts
of data.

In conclusion, the project was a great learning experience. We were able to build an end-to-end data pipeline that automates the process of data
transformation and visualization. We were able to extract valuable insights from the data and visualize them using Power BI. We learned that
the most meaningful insights are extracted when data is corelated across multiple dimensions. We also learned that cloud providers like Azure
provide the necessary tools to build end-to-end data pipelines and that the tools are integrated and work seamlessly together.

\section{Implementation}
\label{sec:implementation}

The end-to-end data pipeline was implemented using Azure Data Factory, Azure Databricks, Azure Synapse Analytics, and Power BI. 
The pipeline consists of the following stages: data ingestion, data preprocessing, data integration, and data visualization. 
The implementation steps are described below.

\subsection{Dataset Used: IMDb Dataset}

The IMDb dataset contains information about the film industry and consists of five tables:
\begin{enumerate}
    \item \textbf{name\_basics}: Includes information about individuals, such as names, birth and death dates, occupations, and associated movies.
    \item \textbf{title\_basics}: Contains details about movies, such as type, title, release year, duration, and genres.
    \item \textbf{title\_akas}: Offers alternative titles and translations of the title in other languages.
    \item \textbf{title\_principals}: Specifies the roles of each person involved in creating the movie, including details about actors' roles.
    \item \textbf{title\_ratings}: Comprises IMDb movie scores and the number of reviews.
\end{enumerate}

Link to the dataset: \href{https://www.kaggle.com/datasets/ashirwadsangwan/imdb-dataset/data}{IMDb Dataset}

\subsection{Creating a Data Lake}
The first step involved building a data lake, a storage infrastructure designed for managing large volumes of data. Two containers were created:
\begin{itemize}
    \item \textbf{bronze}: Used for storing raw, unmodified data.
    \item \textbf{silver}: Used for storing preprocessed data.
\end{itemize}

\subsection{Configuring Access to the Data Lake}

Using a dedicated notebook, \texttt{mountStorage.ipynb}, the containers were mounted in 
Azure Databricks, a scalable data processing platform. This configuration enabled direct 
access to files stored in the data lake from notebooks.

\subsection{Downloading and Storing Raw Data}

The dataset download process was implemented in the notebook \texttt{imdb2bronze.ipynb}. The data 
was obtained from the Kaggle platform using the official library in a Jupyter Notebook environment 
within Azure Databricks. The data, available as TSV (Tab Separated Values) files, was read using 
Apache Spark and stored in the \textbf{bronze} container in Parquet format.

\subsection{Data Preprocessing}

Data preprocessing is a critical step in preparing data for analysis. This process was 
implemented in the notebook \texttt{bronze2silver.ipynb}, including the following steps:

\begin{itemize}
    \item \textbf{Column name transformation}: Conversion of column names to \texttt{snake\_case}.
    \item \textbf{Null value replacement}: The string \texttt{\textbackslash N}, used in the original dataset for missing values, was replaced with \texttt{NULL}.
    \item \textbf{Data type conversion}: Columns containing numbers stored as strings were converted to appropriate numeric types (e.g., \texttt{int} or \texttt{decimal}).
    \item \textbf{Redundancy elimination}: Columns that did not add value to the analysis were removed.
    \item \textbf{Data normalization}: Columns containing lists of values were split into new tables, improving the structure and integrity of the data.
\end{itemize}

The preprocessed data was saved in the \textbf{silver} container in Delta format.

\subsection{Creating SQL Views in Azure Synapse}

For each table in the \textbf{silver} container, views were created in Azure Synapse Analytics. This step facilitated data access and integration in subsequent project stages.

\section{Conclusions}
\label{sec:conclusions}

As a result of the project, we were able to extract following insights from the IMDb dataset:

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|p{10cm}|}
    \hline
    \textbf{Analysis Aspect} & \textbf{Details} \\
    \hline
    \textbf{Best rated genres} & History, Documentary, Biography, Animation \\
    \hline
    \textbf{Distribution of movies (year)} & Peak in 2021 (501k movies) \\
    \hline
    \textbf{Most popular genres} & Drama, Comedy, Talk-Show \\
    \hline
    \textbf{Rating distribution} & Gaussian distribution with mean \(\sim 7.4\) \\
    \hline
    \textbf{Runtime average by genre} & Film-Noir (82 mins), Adult (79 mins), Sport (63 mins) \\
    \hline
    \textbf{Average rating by runtime} & Peak at 40 mins (7.41), dips to \(\sim 6\) at 85 mins, then gradually rises to \(\sim 7.45\) at 285 mins \\
    \hline
    \textbf{Distribution by runtime} & Peaks at 20 mins (450k movies), 30 mins (374k), 60 mins (273k) \\
    \hline
    \textbf{Average votes by runtime} & Bell-shaped curve centered \(\sim 90\)–200 mins, peak at 170 mins (13k votes) \\
    \hline
    \textbf{Count of movies by region} & US (946k), GB (118k), IN (334k), CA (194k), FR (185k) \\
    \hline
    \end{tabular}
    \caption{Concatenated Table of Analysis Aspects}
    \label{tab:analysis_aspects}
\end{table}
    
Following are some captures of the Power BI dashboard created for the IMDb dataset:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../assets/movies_summary.png}
    \caption{Summary of Movies Analysis}
    \label{fig:movies_summary}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../assets/movies_runtime.png}
    \caption{Runtime Analysis of Movies}
    \label{fig:movies_runtime}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../assets/movies_region.png}
    \caption{Region-wise Distribution of Movies}
    \label{fig:movies_region}
\end{figure}

% References Section
\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}
